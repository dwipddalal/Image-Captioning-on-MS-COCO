{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCHaEn-IZYYw"
   },
   "source": [
    "# Importing necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ERJpchvZZYY0"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "smBsUlIvZYY1",
    "outputId": "d91067a2-590f-48fa-f91a-9a7953389ddc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dwip.dalal/Openvivo/Text to image generation'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.abspath('.') # gives current path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIuyd6wyZYY2"
   },
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "P3dmInYZZYY3"
   },
   "outputs": [],
   "source": [
    "dataset = os.path.abspath('.') + \"/annotations/captions_train2014.json\"\n",
    "with open(dataset, 'r') as f:\n",
    "    data = json.load(f) #loaded the annotations file in annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D2jOlyBUZYY3",
    "outputId": "026b92fa-0fe6-46d9-eca1-f323d24a08f7"
   },
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCx4mca0ZYY4"
   },
   "source": [
    "### Let's make a dictionary that stores images paths as key and captions as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhgwYskUZYY4"
   },
   "outputs": [],
   "source": [
    "image_path_to_caption = collections.defaultdict(list)\n",
    "for val in data['annotations']:\n",
    "  caption = f\"<start> {val['caption']} <end>\"\n",
    "  image_path =  os.path.abspath('.') + '/train2014/' + 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])\n",
    "  image_path_to_caption[image_path].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cICuEQN3ZYY5",
    "outputId": "c39bdc12-b5fa-4aa2-b590-ad10737c02d7"
   },
   "outputs": [],
   "source": [
    "print(next(iter(image_path_to_caption.keys()))) #here we can see that the list of image path forms the key of image_path_To_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CoHL8bQPZYY5",
    "outputId": "223fdab6-99b0-4bfa-a7ff-4f1b86b466c5"
   },
   "outputs": [],
   "source": [
    "len(list(image_path_to_caption.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGf-ts-AZYY6"
   },
   "source": [
    "### defining training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ndY7Y8fZYY9"
   },
   "outputs": [],
   "source": [
    "train_image_paths = list(image_path_to_caption.keys())[:3000]\n",
    "random.shuffle(train_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2Jn1sddZYY9"
   },
   "outputs": [],
   "source": [
    "train_captions = []\n",
    "image_vector = []\n",
    "\n",
    "for image_path in train_image_paths:\n",
    "  caption_list = image_path_to_caption[image_path]\n",
    "  train_captions.extend(caption_list)\n",
    "  image_vector.extend([image_path]*len(caption_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I70DMrtTZYY9",
    "outputId": "00c5e4bd-4542-4cd0-8d28-8d64e9272574"
   },
   "outputs": [],
   "source": [
    "print(image_vector[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jYBrU-NJZYY-",
    "outputId": "3c5628f6-21b6-484e-8320-4aefab125900"
   },
   "outputs": [],
   "source": [
    "Image.open(image_vector[0])\n",
    "print(train_captions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDqjEibFZYY-"
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XARVxMhMZYY-",
    "outputId": "df10e46b-7965-4435-9c30-454075740fb0"
   },
   "outputs": [],
   "source": [
    "caption_dataset = tf.data.Dataset.from_tensor_slices(train_captions)\n",
    "max_length = 50\n",
    "vocabulary_size = 5000\n",
    "tokenizer = tf.keras.layers.TextVectorization(max_tokens=vocabulary_size, output_sequence_length=max_length)\n",
    "tokenizer.adapt(caption_dataset)\n",
    "caption_vec = caption_dataset.map(lambda x: tokenizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVQMekx4ZYY_"
   },
   "outputs": [],
   "source": [
    "word_to_index = tf.keras.layers.StringLookup(mask_token=\"\", vocabulary=tokenizer.get_vocabulary())\n",
    "index_to_word = tf.keras.layers.StringLookup(mask_token=\"\", vocabulary=tokenizer.get_vocabulary(), invert=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXrsZoVxlmih"
   },
   "source": [
    "### Loading inceptionV2 and passing image through it so as to get abstract representation of the image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K_HvD2lFZYY_"
   },
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.keras.layers.Resizing(299, 299)(img)  # since we shall use InceptionV3 \n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHn7ngdIZYY_",
    "outputId": "93446f82-ec69-4ff6-d2ee-c68cad427213"
   },
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
    "\n",
    "encode_train = sorted(set(image_vector))\n",
    "\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n",
    "\n",
    "for img, path in image_dataset:\n",
    "  batch_features = image_features_extract_model(img)\n",
    "  batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "\n",
    "  for bf, p in zip(batch_features, path):\n",
    "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "    np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Id1_p8tQnFkD"
   },
   "source": [
    "# Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDIng5UoZYZA"
   },
   "outputs": [],
   "source": [
    "img_to_caption_vec = collections.defaultdict(list)\n",
    "for img, cap in zip(image_vector, caption_vec):\n",
    "  img_to_caption_vec[img].append(cap)\n",
    "\n",
    "img_keys = list(img_to_caption_vec.keys())\n",
    "random.shuffle(img_keys)\n",
    "\n",
    "image_name_train_keys, image_name_val_keys = img_keys[:int(len(img_keys)*0.8)], img_keys[int(len(img_keys)*0.8):]\n",
    "\n",
    "image_name_train = []\n",
    "caption_train = []\n",
    "\n",
    "for imgt in image_name_train_keys:\n",
    "  capt_len = len(img_to_caption_vec[imgt])\n",
    "  image_name_train.extend([imgt] * capt_len)\n",
    "  caption_train.extend(img_to_caption_vec[imgt])\n",
    "\n",
    "image_name_val = []\n",
    "cap_val = []\n",
    "for imgv in image_name_val_keys:\n",
    "  capv_len = len(img_to_caption_vec[imgv])\n",
    "  image_name_val.extend([imgv] * capv_len)\n",
    "  cap_val.extend(img_to_caption_vec[imgv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wII4YKHmZYZA",
    "outputId": "c5f1e956-62a2-4335-fb1b-bad8756221cd"
   },
   "outputs": [],
   "source": [
    "len(image_name_train), len(caption_train), len(image_name_val), len(cap_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x6Mwp1xVZYZB"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "num_steps = len(image_name_train) // BATCH_SIZE\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-_1lJ0dZYZB"
   },
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "def map_func(image_name, cap):\n",
    "  img_tensor = np.load(image_name.decode('utf-8')+'.npy')\n",
    "  return img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBYVnLAuZYZB"
   },
   "outputs": [],
   "source": [
    "len(image_name_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8GoRL7oZYZB"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((image_name_train, caption_train))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int64]),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M34736s9ncMK"
   },
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8e5O6sEdZYZB"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, features, hidden):  \n",
    "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "    attention_hidden_layer = (tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis)))\n",
    "    score = self.V(attention_hidden_layer)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "    context_vector = attention_weights * features\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ND07OljOZYZC"
   },
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ta1xa8dCZYZC"
   },
   "outputs": [],
   "source": [
    "class RNN(tf.keras.Model):\n",
    "  def __init__(self, embedding_dim, units, vocab_size):\n",
    "    super(RNN, self).__init__()\n",
    "    self.units = units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "    self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "  def call(self, x, features, hidden):\n",
    "    context_vector, attention_weights = self.attention(features, hidden)\n",
    "    x = self.embedding(x)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "    output, state = self.gru(x)\n",
    "    x = self.fc1(output)\n",
    "    x = tf.reshape(x, (-1, x.shape[2]))\n",
    "    x = self.fc2(x)\n",
    "\n",
    "    return x, state, attention_weights\n",
    "\n",
    "  def reset_state(self, batch_size):\n",
    "    return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6AIos1LZZYZC"
   },
   "outputs": [],
   "source": [
    "encoder = CNN(embedding_dim)\n",
    "decoder = RNN(embedding_dim, units, tokenizer.vocabulary_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mMrUTAVZYZC"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jL_FkhoZYZC"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder, decoder=decoder, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cKHs1INiZYZD"
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2aqNcH5ZYZD"
   },
   "outputs": [],
   "source": [
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRL_O78BZYZD"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "  loss = 0\n",
    "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "  dec_input = tf.expand_dims([word_to_index('<start>')] * target.shape[0], 1)\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "      features = encoder(img_tensor)\n",
    "\n",
    "      for i in range(1, target.shape[1]):\n",
    "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "          loss += loss_function(target[:, i], predictions)\n",
    "          dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "  total_loss = (loss / int(target.shape[1]))\n",
    "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "  gradients = tape.gradient(loss, trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "  return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIF-mh9VZYZD"
   },
   "outputs": [],
   "source": [
    "# for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "#         print(img_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpWGbtDeZYZD",
    "outputId": "caa9ead1-cd40-4263-a778-39356779354f"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "#         print(img_tensor.shape, target.shape)\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "      ckpt_manager.save()\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eGLsKLEZYZE",
    "outputId": "8c660a5c-25c0-40f5-da02-aecf2b6cb679"
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cssmBs0JZYZE"
   },
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([word_to_index('<start>')], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        predicted_word = tf.compat.as_text(index_to_word(predicted_id).numpy())\n",
    "        result.append(predicted_word)\n",
    "\n",
    "        if predicted_word == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkDlE8vTZYZE"
   },
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for i in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[i], (8, 8))\n",
    "        grid_size = max(int(np.ceil(len_result/2)), 2)\n",
    "        ax = fig.add_subplot(grid_size, grid_size, i+1)\n",
    "        ax.set_title(result[i])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56twy95EZYZE",
    "outputId": "597a052b-2a9a-4d0d-c1c1-f82ebaf2dc61"
   },
   "outputs": [],
   "source": [
    "# captions on the validation set\n",
    "rid = np.random.randint(0, len(image_name_val))\n",
    "image = image_name_val[rid]\n",
    "real_caption = ' '.join([tf.compat.as_text(index_to_word(i).numpy()) for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "print(result)\n",
    "print('Real Caption:', real_caption[5:-3])\n",
    "print('Prediction Caption:', ' '.join(result))\n",
    "Image.open(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:pal]",
   "language": "python",
   "name": "conda-env-pal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
